# Home_Sales
Module 22 Challenge - Big Data, PySpark

## Table of Contents

- About
- Getting Started
- Installing
- Contributing

## About
Using PySpark libraries, this topic covers the essential module extensions that will help us aggregate, transform, and save data from big data sources.

Methods of partitioning and caching will also be used to demonstrate the capabiility of improving performance speeds when manipulating data to create more efficient work processes. 

## Getting Started
Using Google Colab will allow us to import all memory intensive packages such as java, pyspark, and hadoop to be able to work with big data sources in a cloud based environment.

Simply:
    - go to https://colab.google/
    - "Open Colab"
    - Upload the starter file ("Home_Sales_starter_code_colab.ipynb") to the enviornment to get started

## Installing 
Install dependencies by initializing them in the kernel.
Dependencies included:
    - findspark
    - pySpark, pySpark.sql

## Contributing
Contributions made by Andrew S. (TA) on how to setup dependencies through Windows OS.